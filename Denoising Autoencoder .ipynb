{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from  tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mnist = r'C:/Users/utkar/Desktop/ML/Dataset'\n",
    "\n",
    "train = datasets.MNIST(\n",
    "    path_mnist, \n",
    "    train=True, \n",
    "    download=False, \n",
    "    transform=transforms.Compose([transforms.ToTensor(),])\n",
    ")\n",
    "test = datasets.MNIST(\n",
    "    path_mnist, \n",
    "    train=False, \n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(),])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, \n",
    "    batch_size=64, \n",
    "    pin_memory=True, \n",
    "    num_workers=4, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, \n",
    "    batch_size=64, \n",
    "    pin_memory=True, \n",
    "    num_workers=4, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe870db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader.dataset, test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoiseAutoencoder, self).__init__()\n",
    "        self.pool = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 8, kernel_size=3, padding=1)   # instead of stride of 2 use maxpool it is good\n",
    "        \n",
    "        self.up_conv1 = nn.ConvTranspose2d(8, 32, kernel_size=3, stride=2) \n",
    "        self.up_conv2 = nn.ConvTranspose2d(32, 64, kernel_size=2, stride=2) \n",
    "        self.up_conv3 = nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2) \n",
    "        #self.up_conv4 = nn.ConvTranspose2d(32, 1, kernel_size=2, stride=2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = F.relu(self.up_conv1(x))\n",
    "        x = F.relu(self.up_conv2(x))\n",
    "        x = torch.sigmoid(self.up_conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caaa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenoiseAutoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59befed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(torchinfo.summary(model, input_size=(1, 1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ed46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scaler = torch.cuda.amp.GradScaler()   # to avoid vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc45aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# for adding noise to images\n",
    "noise_factor=0.5\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    loop = tqdm(train_loader)\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in loop:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        images, _ = data\n",
    "        \n",
    "        ## add random noise to the input images\n",
    "        noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
    "        # Clip the images to be between 0 and 1\n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        \n",
    "        noisy_imgs = noisy_imgs.to('cuda')\n",
    "        images = images.to('cuda')\n",
    "        \n",
    "        ## forward pass: compute predicted outputs by passing *noisy* images to the model\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(noisy_imgs)\n",
    "            # calculate the loss\n",
    "            # the \"target\" is still the original, not-noisy images\n",
    "            loss = criterion(outputs, images)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        \n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        img = (images.cpu()[2].permute(1, 2, 0)+1)/2\n",
    "        gen = (outputs.cpu()[2].permute(1, 2, 0)+1)/2\n",
    "        img = np.concatenate((img, gen), axis=1)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
